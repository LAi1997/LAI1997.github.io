---
---

@article{ai_4_eb,
    abbr = {AAAI SSS23},
    bibtex_show = {true},
    author = {L. Ai and S.-S. Liang and W.-Z. Dai and L. Hallett and S. H. Muggleton and G. S. Baldwin.},
    year = {2023},
    title = {Human comprehensible active learning of genome-scale metabolic network},
    journal = {AAAI 2023 Spring Symposium on Computational Approaches to Scientific Discovery},
    html = {http://cogsys.org/symposium/discovery-2023/abstracts/Abstract_3169.pdf},
    note = {I developed a new AI framework called ILP-iML1515 that could be used to efficiently learn new gene functions and navigate large metabolic networks. This research has been funded by the AI for Engineering Biology (AI-4-EB) project under BBSRC, and led by Prof. Stephen Muggleton in the Department of Computing and Prof. Geoff Baldwin in the Department of Life Sciences at ICL.},
}

@article{sequential_teaching,
    abbr = {MLJ},
    bibtex_show = {true},
    author = {L. Ai and J. Langer and S. H. Muggleton and U. Schmid},
    year = {2023},
    title = {Explanatory machine learning for sequential human teaching},
    journal = {Machine Learning},
    doi = {https://doi.org/10.1007/s10994-023-06351-8},
    html = {https://link.springer.com/article/10.1007/s10994-023-06351-8},
    note = {I extended the our framework to examine the effects of AI explanations (learned via Inductive Logic Programming) on human comprehension in sequential machine-human interactions. Our analysis demonstrated the potential of AI explanations to facilitate human discovery of computational algorithms and optimised problem-solving strategies. },
}

@article{beneficial_harmful,
    abbr = {MLJ},
    bibtex_show = {true},
    author = {L. Ai and S. H. Muggleton and C. Hocquette and M. Gromowski and U. Schmid},
    year = {2021},
    title = {Beneficial and harmful explanatory machine learning},
    journal = {Machine Learning},
    doi = {https://doi.org/10.1007/s10994-020-05941-0},
    html = {https://link.springer.com/article/10.1007/s10994-020-05941-0},
    note = {I contributed to Explainable Artificial Intelligence by formulating our new framework to assess human comprehension of AI explanations (learned via Inductive Logic Programming) and provided evidence of quantified changes in human comprehension},
}

@phdthesis{lai_thesis,
    abbr = {PhD Thesis}
    title = {Effects of Machine-Learned Logic Theories on Human Comprehension in Machine-Human Teaching},
    author = {Lun Ai},
    year = 2024,
    html = {https://doi.org/10.25560/112264},
    note = {Imperial College London},
    school = {Imperial College London},
    type = {PhD thesis},
    notes = {In my PhD, I aimed to bring accountability to the effects of human-AI interactions. The majority of AI systems remain frustratingly opaque to human comprehension. Explainable Artificial Intelligence seeks to bridge this gap by exploring AI systems that explain their inner structures, decisions, and functions to humans. Yet, a critical challenge persists in establishing a framework for assessing how well people can comprehend AI due to the lack of clear definitions and experimental procedures. To address this issue, I developed a framework to evaluate the human comprehension of AI explanations in the context of Inductive Logic Programming.},
}